function [nSplitCross,nBootCross,nSplitGen,nBootGen,nAIC] = ...	ModelComparisonTutorial(theta,nFit,noiseSd,nX,nReplications)% [nSplitCross,nBootCross,nSplitGen,nBootGen] = ...%		ModelComparisonTutorial([theta],[nFit],[noiseSd][nX,[nReplications])%% Little excercises in various methods of model comparison.% Data are generated according to a polynomial of known% order, and then we try to find the best "correct" model% during data analysis.%% Script generates data y = f(x) accordiing to a known% polynomial and then fits with polynomials of various% orders.  The question explored is how well various model% comparison methods select the "right" order of polynomial% from amongst the various choices.%% Currently implemented methods are based on cross-validation% and generalization.%% You can either call this as an engine or run it stand-alone.% Default values are filled in for args not passed or passed% as [].%% Fitting is done as simple least squares, using MATLAB's% regression (\) operator.%% Interesting extensions of this might include:%		i) Changing to maximum likelihood fitting and%		  seeing how well AIC performs.%		ii) Implementing non-normal error and seeing what%		  happens.% 	iii) Exploring different ways of dividing the data%		  for either cross-validation or generalization computations.%   iv) Adding another loop around the outside that systematically% 	  examines effect of parameters such as noise level or degree%		  of underlying polynomial.%   v) Adding different classes of model to the mix.  These are%		  all nested within the most complex.%   vi) Exploring effect of sampling density on x axis on the%     methods tried here, and of various spacings of independent%     variable along x-axis.%   vii) In cases where the higher order polynomial is returned as%     best model, this script doesn't look at the magnitude of the%     parameters.  But intuitively it seems like the case where they%     are small is a less serious error than the case where they are big.%   viii) Generalization depends on how precisely the parameters are determined.%     One could implement methods for estimating this, and for estimating how%     the parameter uncertainty propogates through to the extrapolation data.%% This script is formatted with tab stops set to 2 and will look% bad for other settings.%% 7/20/04		dhb		Wrote it.% 7/26/04		dhb		Specify x in a range.% 						Make it a callable function, to make it easier%                            to explore systematically the effect of various parameters.% 2/21/06       dhb     Add AIC in SSE form.% Define parameter vector theta.  The length of% theta defines the order of the generating% polynomial, and the entries are the coefficients.% y = theta(1) + theta(2)*x + theta(3)*x^2, etc.if (nargin < 1 | isempty(theta))	theta = [1 3 -2]';end% This parameters determines the order of polynomials% that are tried.  Values up to nFit are used.if (nargin < 2 | isempty(nFit))	nFit = 6;end% The data are generated on an array linearly spaced% between xLow and xHigh, at nX points.% nReplications data sets are generated.  Each is formed% from the underlying "true" data plus normally distributed% noise, with standard deviation noiseSd;if (nargin < 3 | isempty(noiseSd))	noiseSd = 10;endif (nargin < 4 | isempty(nX))	nX = 10;endif (nargin < 5 | isempty(nReplications))	nReplications = 4;end% We'll repead the simulation a bunch of times,% keeping track of how well each of the model% comparison methods works.nSimulations = 100;% You can plot a figure of the data draw each time % through the simulation, but it slows things down.% Set this to 1 to draw the plot and to zero not to.DATAPLOT = 1;% You can also see the fit of polynomials of various% order to the whole dataset.  Again, this is of interest% at first but gets boring once you've seen a few.  And% it slows the simulations down.FITPLOT = 1;% Plot the output histograms?HISTOPLOT = 1;% Some simple checks.  Enforcing that nReplications and% nX be even numbers simplifies the code below.if (rem(nReplications ,2) ~= 0)	error('Code logic requires an even number of replications');endif (rem(nX ,2) ~= 0)	error('Code logic requires an even number of xValues');end% Generate some data just to see what we're dealing with[x,yTrue,yObs,yMean] = GenerateData(theta,nX,nReplications,noiseSd);figure(1); clf; hold onplot(x,yTrue,'g','LineWidth',2);plot(x,yObs,'ro','MarkerSize',4);plot(x,yMean,'ro','MarkerSize',8,'MarkerFaceColor','r');drawnow;% Here's the loop to evalute model comparison performance% multiple times.for sim = 1:nSimulations	% Get degree of polynomial.	nTheta = length(theta);	[x,yTrue,yObs,yMean] = GenerateData(theta,nX,nReplications,noiseSd);	% Plot the data if requested.  This is a bit informative	% to give a sense of the relative size of the measurement	% noise.	%		green line			 		- underlying "true" data	%		red open circles 		- simulated data for each replication	%   red closed circles	- mean of simulated data	if (DATAPLOT)		figure(1); clf; hold on		plot(x,yTrue,'g','LineWidth',2);		plot(x,yObs,'ro','MarkerSize',4);		plot(x,yMean,'ro','MarkerSize',8,'MarkerFaceColor','r');		drawnow;	end		% Model fits to whole data set    predYs = {};    XFit = [];    for i = 1:nFit        XFit = [XFit x.^(i-1)];        thetaHats = XFit\yMean;        predYs{i} = XFit*thetaHats;        residuals = yMean-predYs{i};        SSE = sum(residuals.^2);        AICs(i) = nX*log(SSE/nX)+2*i+2*i*(i+1)/(nX-i-1);    end    [minAIC,minIndex] = min(AICs);    bestAICFits(sim) = minIndex;        % Look at model fits to whole data set	if (DATAPLOT & FITPLOT)		% First fit the observed means with polynomials of various		% order.  Then add linear, true, and highest order fits		% to the data plot.			figure(1);		plot(x,predYs{2},'r-','LineWidth',1);		plot(x,predYs{nTheta},'k-','LineWidth',1);		plot(x,predYs{end},'r-','LineWidth',1);		drawnow;	end		% Split half cross-validation.  This is now coded as	% fit to first half and evaluate on second half, but one	% could try all ways of splitting data set in two and average.	firstHalfIndex = 1:nReplications/2;	secondHalfIndex = nReplications/2+1:nReplications;	yFitMean = mean(yObs(:,firstHalfIndex),2);	yCrossMean = mean(yObs(:,secondHalfIndex),2);	XFit = [];	for i = 1:nFit		XFit = [XFit x.^(i-1)];		thetaHats = XFit\yFitMean;		predYs = XFit*thetaHats;		residuals = yCrossMean-predYs;		crossSSEs(i) = sum(residuals.^2);	end	[minSSE,minIndex] = min(crossSSEs);	bestCrossFits(sim) = minIndex;	% Bootstrap cross-validation.  This goes through	% and leaves out one replication at a time.  The	% model is then fit to the other replications and	% the fit to the one left out is evaluated.  These	% values are then averaged over all the ones left	% out and the mean is taken as the figure of merit.	bootCrossSSEs = zeros(nFit,nReplications);	for j = 1:nReplications		firstHalfIndex = find(1:nReplications ~= j);		secondHalfIndex = j;		yFitMean = mean(yObs(:,firstHalfIndex),2);		yCrossMean = mean(yObs(:,secondHalfIndex),2);		XFit = [];		for i = 1:nFit			XFit = [XFit x.^(i-1)];			thetaHats = XFit\yFitMean;			predYs = XFit*thetaHats;			residuals = yCrossMean-predYs;			bootCrossSSEs(i,j) = sum(residuals.^2);		end	end	[minSSE,minIndex] = min(mean(bootCrossSSEs,2));	bestBootCrossFits(sim) = minIndex;	% Generalization by extrapolation.  Data are fit	% to first half of independent variable and then	% fit is evaluated against second half.	firstHalfIndex = 1:nX/2;	secondHalfIndex = nX/2+1:nX;	yFitMean = mean(yObs(firstHalfIndex,:),2);	yGenMean = mean(yObs(secondHalfIndex,:),2);	XFit = [];	XGen = [];	for i = 1:nFit		XFit = [XFit x(firstHalfIndex).^(i-1)];		XGen = [XGen x(secondHalfIndex).^(i-1)];		thetaHats = XFit\yFitMean;		predYs = XGen*thetaHats;		residuals = yGenMean-predYs;		genSSEs(i) = sum(residuals.^2);		genThetaHats{i}(:,sim) = thetaHats;	end	[minSSE,minIndex] = min(genSSEs);	bestGenFits(sim) = minIndex;	% Bootstrap generalization.  One level of the independent	% variable is left out, and the data from the rest are fit.	% the prediction error to the one left out is evaluated,	% and this value is averaged over the rest.	bootGenSSEs = zeros(nFit,nX);	for j = 1:nX		firstHalfIndex = find(1:nX ~= j);		secondHalfIndex =j;		yFitMean = mean(yObs(firstHalfIndex,:),2);		yGenMean = mean(yObs(secondHalfIndex,:),2);		XFit = [];		XGen = [];		for i = 1:nFit			XFit = [XFit x(firstHalfIndex).^(i-1)];			XGen = [XGen x(secondHalfIndex).^(i-1)];			thetaHats = XFit\yFitMean;			predYs = XGen*thetaHats;			residuals = yGenMean-predYs;			bootGenSSEs(i,j) = sum(residuals.^2);			bootGenThetaHats{i}(:,sim) = thetaHats;		end	end	[minSSE,minIndex] = min(mean(bootGenSSEs,2));	bestBootGenFits(sim) = minIndex;end% Make a figure that shows how often each method identifies% the correct model.  This figure is basically a summary% of model comparison performance.  First step is to % make a histogram of how many times each method picked% each order of polynomial.  Then these are plotted together.% The red vertical line in each panel indicates the correct% model.for i = 1:nFit    nAIC(i) = length(find(bestAICFits == i));	nSplitCross(i) = length(find(bestCrossFits == i));	nBootCross(i) = length(find(bestBootCrossFits == i));	nSplitGen(i) = length(find(bestGenFits == i));	nBootGen(i) = length(find(bestBootGenFits == i));endif (HISTOPLOT)	figure(3); clf;	subplot(5,1,1); hold on	bar(1:nFit,[nSplitCross'])	axis([0 nFit+1 0 nSimulations]);	plot([nTheta nTheta],[0 nSimulations],'r');	title(sprintf('Splithalf Cross Validation'));	subplot(5,1,2); hold on	bar(1:nFit,[nBootCross'])	axis([0 nFit+1 0 nSimulations]);	plot([nTheta nTheta],[0 nSimulations],'r');	title(sprintf('Bootstrap Cross Validation'));	subplot(5,1,3); hold on	bar(1:nFit,[nSplitGen'])	axis([0 nFit+1 0 nSimulations]);	plot([nTheta nTheta],[0 nSimulations],'r');	title(sprintf('Extrapolation Generalization'));	subplot(5,1,4); hold on	bar(1:nFit,[nBootGen'])	axis([0 nFit+1 0 nSimulations]);	plot([nTheta nTheta],[0 nSimulations],'r');	title(sprintf('Bootstrap Generalization'));    subplot(5,1,5); hold on    bar(1:nFit,[nAIC'])	axis([0 nFit+1 0 nSimulations]);	plot([nTheta nTheta],[0 nSimulations],'r');	title(sprintf('AIC'));	drawnow;end% Plot split-half generalization theta estimatesfigure(4); clf;subplot(2,1,1); hold on;plot((1:nTheta)',theta,'ko','MarkerSize',12,'MarkerFaceColor','k');plot((1:nTheta)',genThetaHats{nTheta},'r+');plot((1:nTheta)',bootGenThetaHats{nTheta},'g+');title('True Model Parameter Estimates');axis([1 nTheta+1 -500 500]);% Plot split-half generalization theta estimatessubplot(2,1,2); hold on;plot((1:nTheta+1)',[theta ; 0],'ko','MarkerSize',12,'MarkerFaceColor','k');plot((1:nTheta+1)',genThetaHats{nTheta+1},'r+');plot((1:nTheta+1)',bootGenThetaHats{nTheta+1},'g+');axis([1 nTheta+1 -500 500]);title('Complex Model Parameter Estimates');