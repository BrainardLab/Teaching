% ModelComparisonTutorial%% Little excercises in various methods of model comparison.% Data are generated according to a polynomial of known% order, and then we try to find the best "correct" model% during data analysis.%% Script generates data y = f(x) accordiing to a known% polynomial and then fits with polynomials of various% orders.  The question explored is how well various model% comparison methods select the "right" order of polynomial% from amongst the various choices.%% Currently implemented methods are based on cross-validation% and generalization.%% Fitting is done as simple least squares, using MATLAB's% regression (\) operator.%% Interesting extensions of this might include:%		i) Changing to maximum likelihood fitting and%		  seeing how well AIC performs.%		ii) Implementing non-normal error and seeing what%		  happens.% 	iii) Exploring different ways of dividing the data%		  for either cross-validation or generalization computations.%   iv) Adding another loop around the outside that systematically% 	  examines effect of parameters such as noise level or degree%		  of underlying polynomial.%   v) Adding different classes of model to the mix.  These are%		  all nested within the most complex.%   vi) Exploring effect of sampling density on x axis on the%     methods tried here, and of various spacings of independent%     variable along x-axis.%   vii) In cases where the higher order polynomial is returned as%     best model, this script doesn't look at the magnitude of the%     parameters.  But intuitively it seems like the case where they%     are small is a less serious error than the case where they are big.%   viii) Generalization depends on how precisely the parameters are determined.%     One could implement methods for estimating this, and for estimating how%     the parameter uncertainty propogates through to the extrapolation data.%% This script is formatted with tab stops set to 2 and will look% bad for other settings.%% 7/20/04		dhb		Wrote it.% Clear and closeclear all; close all% Define parameter vector theta.  The length of% theta defines the order of the generating% polynomial, and the entries are the coefficients.% y = theta(1) + theta(2)*x + theta(3)*x^2, etc.theta = [1 3 -2]';% The data are generated on an array of integers 1:nX.% nReplications data sets are generated.  Each is formed% from the underlying "true" data plus normally distributed% noise, with standard deviation noiseSd;nX = 10;noiseSd = 100;nReplications = 4;% This parameters determines the order of polynomials% that are tried.  Values up to nFit are used.nFit = 6;% We'll repead the simulation a bunch of times,% keeping track of how well each of the model% comparison methods works.nSimulations = 100;% You can plot a figure of the data draw each time % through the simulation, but it slows things down.% Set this to 1 to draw the plot and to zero not to.DATAPLOT = 1;% You can also see the fit of polynomials of various% order to the whole dataset.  Again, this is of interest% at first but gets boring once you've seen a few.  And% it slows the simulations down.FITPLOT = 1;% Some simple checks.  Enforcing that nReplications and% nX be even numbers simplifies the code below.if (rem(nReplications ,2) ~= 0)	error('Code logic requires an even number of replications');endif (rem(nX ,2) ~= 0)	error('Code logic requires an even number of xValues');end% Here's the loop to evalute model comparison performance% multiple times.for sim = 1:nSimulations	% Get degree of polynomial.	nTheta = length(theta);		% Define values of x to be used in simulated	% experiment.  It's as easy to use the	% integers 1:nX 	x = (1:nX)';		% Generate noise free data y and make a little plot	X = [];	for i = 1:nTheta		X = [X x.^(i-1)];	end	yTrue = X*theta;		% Now we generate experimental data.  This	% is the true data plus normally distributed noise.	% The observations end up in the columsn of yObs;	noiseDraws = noiseSd*randn(nX,nReplications);	yObs = yTrue(:,ones(1,nReplications))+noiseDraws;	yMean = mean(yObs,2);	% Plot the data if requested.  This is a bit informative	% to give a sense of the relative size of the measurement	% noise.	%		green line			 		- underlying "true" data	%		red open circles 		- simulated data for each replication	%   red closed circles	- mean of simulated data	if (DATAPLOT)		figure(1); clf; hold on		plot(x,yTrue,'g','LineWidth',2);		plot(x,yObs,'ro','MarkerSize',4);		plot(x,yMean,'ro','MarkerSize',8,'MarkerFaceColor','r');		drawnow;	end		% Model fits to whole data set, if it is of interest to look at them.	if (DATAPLOT & FITPLOT)		% First fit the observed means with polynomials of various		% order.  Then add linear, true, and highest order fits		% to the data plot.		predYs = {};		XFit = [];		for i = 1:nFit			XFit = [XFit x.^(i-1)];			thetaHats = XFit\yMean;			predYs{i} = XFit*thetaHats;		end		figure(1);		plot(x,predYs{2},'r-','LineWidth',1);		plot(x,predYs{nTheta},'k-','LineWidth',1);		plot(x,predYs{end},'r-','LineWidth',1);		drawnow;	end		% Split half cross-validation.  This is now coded as	% fit to first half and evaluate on second half, but one	% could try all ways of splitting data set in two and average.	firstHalfIndex = 1:nReplications/2;	secondHalfIndex = nReplications/2+1:nReplications;	yFitMean = mean(yObs(:,firstHalfIndex),2);	yCrossMean = mean(yObs(:,secondHalfIndex),2);	XFit = [];	for i = 1:nFit		XFit = [XFit x.^(i-1)];		thetaHats = XFit\yFitMean;		predYs = XFit*thetaHats;		residuals = yCrossMean-predYs;		crossSSEs(i) = sum(residuals.^2);	end	[minSSE,minIndex] = min(crossSSEs);	bestCrossFits(sim) = minIndex;	% Bootstrap cross-validation.  This goes through	% and leaves out one replication at a time.  The	% model is then fit to the other replications and	% the fit to the one left out is evaluated.  These	% values are then averaged over all the ones left	% out and the mean is taken as the figure of merit.	bootCrossSSEs = zeros(nFit,nReplications);	for j = 1:nReplications		firstHalfIndex = find(1:nReplications ~= j);		secondHalfIndex = j;		yFitMean = mean(yObs(:,firstHalfIndex),2);		yCrossMean = mean(yObs(:,secondHalfIndex),2);		XFit = [];		for i = 1:nFit			XFit = [XFit x.^(i-1)];			thetaHats = XFit\yFitMean;			predYs = XFit*thetaHats;			residuals = yCrossMean-predYs;			bootCrossSSEs(i,j) = sum(residuals.^2);		end	end	[minSSE,minIndex] = min(mean(bootCrossSSEs,2));	bestBootCrossFits(sim) = minIndex;	% Generalization by extrapolation.  Data are fit	% to first half of independent variable and then	% fit is evaluated against second half.	firstHalfIndex = 1:nX/2;	secondHalfIndex = nX/2+1:nX;	yFitMean = mean(yObs(firstHalfIndex,:),2);	yGenMean = mean(yObs(secondHalfIndex,:),2);	XFit = [];	XGen = [];	for i = 1:nFit		XFit = [XFit x(firstHalfIndex).^(i-1)];		XGen = [XGen x(secondHalfIndex).^(i-1)];		thetaHats = XFit\yFitMean;		predYs = XGen*thetaHats;		residuals = yGenMean-predYs;		genSSEs(i) = sum(residuals.^2);	end	[minSSE,minIndex] = min(genSSEs);	bestGenFits(sim) = minIndex;	% Bootstrap generalization.  One level of the independent	% variable is left out, and the data from the rest are fit.	% the prediction error to the one left out is evaluated,	% and this value is averaged over the rest.	bootGenSSEs = zeros(nFit,nX);	for j = 1:nX		firstHalfIndex = find(1:nX ~= j);		secondHalfIndex =j;		yFitMean = mean(yObs(firstHalfIndex,:),2);		yGenMean = mean(yObs(secondHalfIndex,:),2);		XFit = [];		XGen = [];		for i = 1:nFit			XFit = [XFit x(firstHalfIndex).^(i-1)];			XGen = [XGen x(secondHalfIndex).^(i-1)];			thetaHats = XFit\yFitMean;			predYs = XGen*thetaHats;			residuals = yGenMean-predYs;			bootGenSSEs(i,j) = sum(residuals.^2);		end	end	[minSSE,minIndex] = min(mean(bootGenSSEs,2));	bestBootGenFits(sim) = minIndex;end% Make a figure that shows how often each method identifies% the correct model.  This figure is basically a summary% of model comparison performance.  First step is to % make a histogram of how many times each method picked% each order of polynomial.  Then these are plotted together.% The red vertical line in each panel indicates the correct% model.for i = 1:nFit	nSplitCross(i) = length(find(bestCrossFits == i));	nBootCross(i) = length(find(bestBootCrossFits == i));	nSplitGen(i) = length(find(bestGenFits == i));	nBootGen(i) = length(find(bestBootGenFits == i));endfigure(3); clf;subplot(4,1,1); hold onbar(1:nFit,[nSplitCross'])axis([0 nFit+1 0 nSimulations]);plot([nTheta nTheta],[0 nSimulations],'r');title(sprintf('Splithalf Cross Validation'));subplot(4,1,2); hold onbar(1:nFit,[nBootCross'])axis([0 nFit+1 0 nSimulations]);plot([nTheta nTheta],[0 nSimulations],'r');title(sprintf('Bootstrap Cross Validation'));subplot(4,1,3); hold onbar(1:nFit,[nSplitGen'])axis([0 nFit+1 0 nSimulations]);plot([nTheta nTheta],[0 nSimulations],'r');title(sprintf('Extrapolation Generalization'));subplot(4,1,4); hold onbar(1:nFit,[nBootGen'])axis([0 nFit+1 0 nSimulations]);plot([nTheta nTheta],[0 nSimulations],'r');title(sprintf('Bootstrap Generalization'));